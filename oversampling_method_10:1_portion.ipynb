{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Balanced Oversampling Technique with a 10:1 Target Label Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/11 21:50:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('fraud_detection').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/11 21:50:29 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "col_names = ['User', 'Card', 'Year', 'Month', 'Day', 'Time', 'Amount', 'Use Chip', 'Merchant Name', 'Merchant City', \n",
    "'Merchant State', 'MCC', 'Errors?', 'Is Fraud?', 'Hour', 'Minute', 'Date', 'Day_of_Week']\n",
    "\n",
    "df = spark.read.option(\"delimiter\", \"|\").csv('credit_card_transactions.csv', header=None, inferSchema=True).toDF(*col_names)\n",
    "df = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "categorical_cols = ['Use Chip', 'Day_of_Week']\n",
    "numerical_cols = ['Card', 'Year', 'Month', 'Day', 'Amount', 'MCC', 'Hour', 'Minute']\n",
    "\n",
    "# Indexers for categorical columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+'_indexed') for col in categorical_cols]\n",
    "# Encoders for categorical columns\n",
    "encoders = [OneHotEncoder(inputCol=col+'_indexed', outputCol=col+'_OHE') for col in categorical_cols]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=(numerical_cols + ['Use Chip_OHE', 'Day_of_Week_OHE']),\n",
    "                            outputCol='features')\n",
    "\n",
    "lr_model = LogisticRegression(featuresCol='features', labelCol='Is Fraud?')\n",
    "\n",
    "stages = indexers + encoders + [assembler, lr_model]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_minority(df):\n",
    "    \n",
    "    num_limit = 1000000\n",
    "    fraction = num_limit / df.filter(col('Is Fraud?') == 0).count()\n",
    "\n",
    "    df_normal = df.filter(col('Is Fraud?')==0).sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "    \n",
    "    fraud_count = df.filter(col('Is Fraud?')==1).count()\n",
    "    \n",
    "    balance_ratio = num_limit / fraud_count\n",
    "    \n",
    "    oversampled_minority = df.filter(col('Is Fraud?')==1).sample(withReplacement=True, fraction=(balance_ratio/10), seed=42)\n",
    "    oversampled_df = df_normal.union(oversampled_minority)\n",
    "    \n",
    "    return oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "oversample_balanced_df = oversample_minority(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|Is Fraud?| count|\n",
      "+---------+------+\n",
      "|        0|999655|\n",
      "|        1| 99820|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "oversample_balanced_df.groupBy('Is Fraud?').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oversampled, test_oversampled = oversample_balanced_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/11 21:54:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/06/11 21:54:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "[Stage 124:====================================================>  (25 + 1) / 26]\r"
     ]
    }
   ],
   "source": [
    "lr_oversampled_model = pipeline.fit(train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_oversampled_model.transform(test_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5680184177175814"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Is Fraud?')\n",
    "AUC = my_eval.evaluate(predictions)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 168:=================================================>     (18 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----+\n",
      "|Is Fraud?|   0.0| 1.0|\n",
      "+---------+------+----+\n",
      "|        0|198417|1364|\n",
      "|        1| 17081|2847|\n",
      "+---------+------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrix manually\n",
    "confusion_matrix = predictions.groupBy(\"Is Fraud?\").pivot(\"prediction\").count().na.fill(0).orderBy(\"Is Fraud?\")\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 182:=================================================>     (18 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cm_array1: [Row(Is Fraud?=0, 0.0=198417, 1.0=1364), Row(Is Fraud?=1, 0.0=17081, 1.0=2847)]\n",
      "cm_array2: [dict_values([0, 198417, 1364]), dict_values([1, 17081, 2847])]\n",
      "cm_array3: [[198417, 1364], [17081, 2847]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert confusion matrix to array for visualization\n",
    "cm_array = confusion_matrix.collect()\n",
    "print(f\"cm_array1: {cm_array}\")\n",
    "cm_array = [row.asDict().values() for row in cm_array]\n",
    "print(f\"cm_array2: {cm_array}\")\n",
    "cm_array = [list(row)[1:] for row in cm_array]\n",
    "print(f\"cm_array3: {cm_array}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqMAAAIjCAYAAAA3LxKwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABXaklEQVR4nO3deVxV1frH8e9B5YAD4AzkhEMOOaCWRDleSTQtSSvHxCmH1EScywHNxIs55pSVQ6ZpVlqpqeSQlTiLY3rVNCvFGRFUUNi/P/x58oQpGMetns/79dqv29nr2Wuvfe6l+/CstRcWwzAMAQAAACZwMXsAAAAAcF4kowAAADANySgAAABMQzIKAAAA05CMAgAAwDQkowAAADANySgAAABMQzIKAAAA05CMAgAAwDQkowDu6NChQ2rQoIE8PT1lsVi0dOnSLO3/2LFjslgsmjNnTpb2+zCrW7eu6tata/YwAOC+IBkFHgJHjhxR165dVbJkSbm5ucnDw0PPPvusJk2apCtXrjj03qGhodqzZ4/effddzZs3T08++aRD73c/tW/fXhaLRR4eHrf9Hg8dOiSLxSKLxaL33nsv0/2fOHFCERERio2NzYLRAsCjKbvZAwBwZ8uXL9crr7wiq9Wqdu3aqWLFikpJSdFPP/2k/v37a9++fZo5c6ZD7n3lyhXFxMTo7bffVs+ePR1yj+LFi+vKlSvKkSOHQ/q/m+zZs+vy5cv69ttv9eqrr9q1zZ8/X25ubrp69eo99X3ixAmNGDFCJUqUkL+/f4avW7169T3dDwAeRiSjwAPs6NGjatmypYoXL661a9fKx8fH1tajRw8dPnxYy5cvd9j9z5w5I0ny8vJy2D0sFovc3Nwc1v/dWK1WPfvss/rss8/SJaMLFixQ48aN9eWXX96XsVy+fFk5c+aUq6vrfbkfADwImKYHHmBRUVFKTEzUxx9/bJeI3lS6dGn17t3b9vn69et65513VKpUKVmtVpUoUUJvvfWWkpOT7a4rUaKEmjRpop9++kk1atSQm5ubSpYsqU8++cQWExERoeLFi0uS+vfvL4vFohIlSki6Mb19859vFRERIYvFYncuOjpaNWvWlJeXl3Lnzq2yZcvqrbfesrX/05rRtWvXqlatWsqVK5e8vLzUtGlT/fLLL7e93+HDh9W+fXt5eXnJ09NTHTp00OXLl//5i/2b1q1b67vvvlN8fLzt3NatW3Xo0CG1bt06Xfz58+fVr18/VapUSblz55aHh4caNWqkXbt22WLWr1+vp556SpLUoUMH23T/zeesW7euKlasqO3bt6t27drKmTOn7Xv5+5rR0NBQubm5pXv+4OBg5c2bVydOnMjwswLAg4ZkFHiAffvttypZsqSeeeaZDMV37txZw4YNU7Vq1TRhwgTVqVNHkZGRatmyZbrYw4cP6+WXX9Zzzz2ncePGKW/evGrfvr327dsnSWrWrJkmTJggSWrVqpXmzZuniRMnZmr8+/btU5MmTZScnKyRI0dq3LhxevHFF/Xzzz/f8brvv/9ewcHBOn36tCIiIhQeHq6NGzfq2Wef1bFjx9LFv/rqq7p06ZIiIyP16quvas6cORoxYkSGx9msWTNZLBZ99dVXtnMLFixQuXLlVK1atXTxv/76q5YuXaomTZpo/Pjx6t+/v/bs2aM6derYEsPy5ctr5MiRkqQuXbpo3rx5mjdvnmrXrm3r59y5c2rUqJH8/f01ceJE1atX77bjmzRpkgoWLKjQ0FClpqZKkj744AOtXr1a77//vnx9fTP8rADwwDEAPJAuXrxoSDKaNm2aofjY2FhDktG5c2e78/369TMkGWvXrrWdK168uCHJ2LBhg+3c6dOnDavVavTt29d27ujRo4YkY+zYsXZ9hoaGGsWLF083huHDhxu3/mtlwoQJhiTjzJkz/zjum/eYPXu27Zy/v79RqFAh49y5c7Zzu3btMlxcXIx27dqlu1/Hjh3t+nzppZeM/Pnz/+M9b32OXLlyGYZhGC+//LJRv359wzAMIzU11fD29jZGjBhx2+/g6tWrRmpqarrnsFqtxsiRI23ntm7dmu7ZbqpTp44hyZgxY8Zt2+rUqWN3btWqVYYkY9SoUcavv/5q5M6d2wgJCbnrMwLAg47KKPCASkhIkCTlyZMnQ/ErVqyQJIWHh9ud79u3rySlW1taoUIF1apVy/a5YMGCKlu2rH799dd7HvPf3Vxr+vXXXystLS1D15w8eVKxsbFq37698uXLZztfuXJlPffcc7bnvFW3bt3sPteqVUvnzp2zfYcZ0bp1a61fv15xcXFau3at4uLibjtFL91YZ+ricuNfn6mpqTp37pxtCcKOHTsyfE+r1aoOHTpkKLZBgwbq2rWrRo4cqWbNmsnNzU0ffPBBhu8FAA8qklHgAeXh4SFJunTpUobif/vtN7m4uKh06dJ25729veXl5aXffvvN7nyxYsXS9ZE3b15duHDhHkecXosWLfTss8+qc+fOKly4sFq2bKnPP//8jonpzXGWLVs2XVv58uV19uxZJSUl2Z3/+7PkzZtXkjL1LM8//7zy5MmjRYsWaf78+XrqqafSfZc3paWlacKECSpTpoysVqsKFCigggULavfu3bp48WKG7/nYY49l6mWl9957T/ny5VNsbKwmT56sQoUKZfhaAHhQkYwCDygPDw/5+vpq7969mbru7y8Q/ZNs2bLd9rxhGPd8j5vrGW9yd3fXhg0b9P333+u1117T7t271aJFCz333HPpYv+Nf/MsN1mtVjVr1kxz587VkiVL/rEqKkmjR49WeHi4ateurU8//VSrVq1SdHS0nnjiiQxXgKUb309m7Ny5U6dPn5Yk7dmzJ1PXAsCDimQUeIA1adJER44cUUxMzF1jixcvrrS0NB06dMju/KlTpxQfH297Mz4r5M2b1+7N85v+Xn2VJBcXF9WvX1/jx4/X/v379e6772rt2rVat27dbfu+Oc6DBw+maztw4IAKFCigXLly/bsH+AetW7fWzp07denSpdu+9HXTF198oXr16unjjz9Wy5Yt1aBBAwUFBaX7TjL6i0FGJCUlqUOHDqpQoYK6dOmiqKgobd26Ncv6BwCzkIwCD7ABAwYoV65c6ty5s06dOpWu/ciRI5o0aZKkG9PMktK98T5+/HhJUuPGjbNsXKVKldLFixe1e/du27mTJ09qyZIldnHnz59Pd+3Nzd//vt3UTT4+PvL399fcuXPtkru9e/dq9erVtud0hHr16umdd97RlClT5O3t/Y9x2bJlS1d1Xbx4sf7880+7czeT5tsl7pk1cOBAHT9+XHPnztX48eNVokQJhYaG/uP3CAAPCza9Bx5gpUqV0oIFC9SiRQuVL1/e7i8wbdy4UYsXL1b79u0lSVWqVFFoaKhmzpyp+Ph41alTR1u2bNHcuXMVEhLyj9sG3YuWLVtq4MCBeumll/Tmm2/q8uXLmj59uh5//HG7F3hGjhypDRs2qHHjxipevLhOnz6tadOmqUiRIqpZs+Y/9j927Fg1atRIgYGB6tSpk65cuaL3339fnp6eioiIyLLn+DsXFxcNGTLkrnFNmjTRyJEj1aFDBz3zzDPas2eP5s+fr5IlS9rFlSpVSl5eXpoxY4by5MmjXLlyKSAgQH5+fpka19q1azVt2jQNHz7cttXU7NmzVbduXQ0dOlRRUVGZ6g8AHiRURoEH3Isvvqjdu3fr5Zdf1tdff60ePXpo0KBBOnbsmMaNG6fJkyfbYj/66CONGDFCW7duVVhYmNauXavBgwdr4cKFWTqm/Pnza8mSJcqZM6cGDBiguXPnKjIyUi+88EK6sRcrVkyzZs1Sjx49NHXqVNWuXVtr166Vp6fnP/YfFBSklStXKn/+/Bo2bJjee+89Pf300/r5558zncg5wltvvaW+fftq1apV6t27t3bs2KHly5eraNGidnE5cuTQ3LlzlS1bNnXr1k2tWrXSDz/8kKl7Xbp0SR07dlTVqlX19ttv287XqlVLvXv31rhx47Rp06YseS4AMIPFyMwKfwAAACALURkFAACAaUhGAQAAYBqSUQAAAJiGZBQAAACmIRkFAACAaUhGAQAAYBqSUQAAAJjmkfwLTO5Ve5o9BAAOcmHrFLOHAMBB3EzMShyZO1zZyb+37oTKKAAAAEzzSFZGAQAAMsVCfc4sJKMAAAAWi9kjcFr8GgAAAADTUBkFAABgmt40fPMAAAAwDZVRAAAA1oyahsooAAAATENlFAAAgDWjpuGbBwAAgGmojAIAALBm1DQkowAAAEzTm4ZvHgAAAKahMgoAAMA0vWmojAIAAMA0VEYBAABYM2oavnkAAACYhsooAAAAa0ZNQ2UUAAAApqEyCgAAwJpR05CMAgAAME1vGn4NAAAAgGmojAIAADBNbxq+eQAAAJiGyigAAACVUdPwzQMAAMA0VEYBAABceJveLFRGAQAAYBoqowAAAKwZNQ3JKAAAAJvem4ZfAwAAAGAaKqMAAABM05uGbx4AAACmoTIKAADAmlHTUBkFAACAaaiMAgAAsGbUNHzzAAAAMA2VUQAAANaMmoZkFAAAgGl60/DNAwAAwDRURgEAAJimNw2VUQAAAJiGyigAAABrRk3DNw8AAADTUBkFAABgzahpqIwCAADANFRGAQAAWDNqGpJRAAAAklHT8M0DAADANFRGAQAAeIHJNFRGAQAAHiAbNmzQCy+8IF9fX1ksFi1dutSu3WKx3PYYO3asLaZEiRLp2seMGWPXz+7du1WrVi25ubmpaNGiioqKSjeWxYsXq1y5cnJzc1OlSpW0YsUKu3bDMDRs2DD5+PjI3d1dQUFBOnToUKael2QUAADA4uK4I5OSkpJUpUoVTZ069bbtJ0+etDtmzZoli8Wi5s2b28WNHDnSLq5Xr162toSEBDVo0EDFixfX9u3bNXbsWEVERGjmzJm2mI0bN6pVq1bq1KmTdu7cqZCQEIWEhGjv3r22mKioKE2ePFkzZszQ5s2blStXLgUHB+vq1asZfl6LYRhGhqMfEu5Ve5o9BAAOcmHrFLOHAMBB3ExcPOje9AOH9X3l6673fK3FYtGSJUsUEhLyjzEhISG6dOmS1qxZYztXokQJhYWFKSws7LbXTJ8+XW+//bbi4uLk6uoqSRo0aJCWLl2qAwcOSJJatGihpKQkLVu2zHbd008/LX9/f82YMUOGYcjX11d9+/ZVv379JEkXL15U4cKFNWfOHLVs2TJDz0hlFAAAwGJx2JGcnKyEhAS7Izk5OUuGferUKS1fvlydOnVK1zZmzBjlz59fVatW1dixY3X9+nVbW0xMjGrXrm1LRCUpODhYBw8e1IULF2wxQUFBdn0GBwcrJiZGknT06FHFxcXZxXh6eiogIMAWkxEkowAAAA4UGRkpT09PuyMyMjJL+p47d67y5MmjZs2a2Z1/8803tXDhQq1bt05du3bV6NGjNWDAAFt7XFycChcubHfNzc9xcXF3jLm1/dbrbheTEbxNDwAA4MB9RgcPHqzw8HC7c1arNUv6njVrltq0aSM3Nze787fer3LlynJ1dVXXrl0VGRmZZffOKlRGAQAAHDhNb7Va5eHhYXdkRUL4448/6uDBg+rcufNdYwMCAnT9+nUdO3ZMkuTt7a1Tp07Zxdz87O3tfceYW9tvve52MRlBMgoAAPAQ+vjjj1W9enVVqVLlrrGxsbFycXFRoUKFJEmBgYHasGGDrl27ZouJjo5W2bJllTdvXlvMrS9F3YwJDAyUJPn5+cnb29suJiEhQZs3b7bFZATT9AAAwOlZHqBN7xMTE3X48GHb56NHjyo2Nlb58uVTsWLFJN1I+hYvXqxx48aluz4mJkabN29WvXr1lCdPHsXExKhPnz5q27atLdFs3bq1RowYoU6dOmngwIHau3evJk2apAkTJtj66d27t+rUqaNx48apcePGWrhwobZt22bb/slisSgsLEyjRo1SmTJl5Ofnp6FDh8rX1/eOb///HckoAADAA2Tbtm2qV6+e7fPN9Z+hoaGaM2eOJGnhwoUyDEOtWrVKd73VatXChQsVERGh5ORk+fn5qU+fPnbrSD09PbV69Wr16NFD1atXV4ECBTRs2DB16dLFFvPMM89owYIFGjJkiN566y2VKVNGS5cuVcWKFW0xAwYMUFJSkrp06aL4+HjVrFlTK1euTLeG9U7YZxTAQ4V9RoFHl5n7jOZ6ebbD+k76ooPD+n4UsGYUAAAApmGaHgAA4MFZMup0qIwCAADANFRGAQCA03uQ3qZ3NiSjAADA6ZGMmodpegAAAJiGyigAAHB6VEbNQ2UUAAAApqEyCgAAnB6VUfNQGQUAAIBpqIwCAABQGDUNlVEAAACYhsooAABweqwZNQ+VUQAAAJiGyigAAHB6VEbNQzIKAACcHsmoeZimBwAAgGmojAIAAKdHZdQ8VEYBAABgGiqjAAAAFEZNQ2UUAAAApqEyCgAAnB5rRs1DZRQAAACmoTIKAACcHpVR85CMAgAAp0cyah6m6QEAAGAaKqMAAAAURk1DZRQAAACmoTIKAACcHmtGzUNlFAAAAKahMgoAAJwelVHzUBkFAACAaaiMAgAAp0dl1DwkowAAwOmRjJqHaXoAAACYhsooAAAAhVHTUBkFAACAaaiMAgAAp8eaUfNQGQUAAIBpqIwCAACnR2XUPFRGAQAAYBoqowAAwOlRGTUPySgAAAC5qGmYpgcAAIBpqIwCAACnxzS9eaiMAgAAwDRURgEAgNOjMmoeKqMAAAAPkA0bNuiFF16Qr6+vLBaLli5datfevn17WSwWu6Nhw4Z2MefPn1ebNm3k4eEhLy8vderUSYmJiXYxu3fvVq1ateTm5qaiRYsqKioq3VgWL16scuXKyc3NTZUqVdKKFSvs2g3D0LBhw+Tj4yN3d3cFBQXp0KFDmXpeklE41LPVSumLiV316+p3dWXnFL1Qt7Jde6F8eTRzRFv9uvpdnds4Xl9PeUOlihW0iymcP48+fqedjkaP1tmN47RxwUCF1Pe/7f1cc2TXpoWDdGXnFFV+/DHbeatrds0c0VZbP39Ll7ZO0ufjX0937cwRbXVl55R0x/Yv3v73XwTgpLZv26peb3RTUN2aqvJEWa1d871d+/Sp76tpk4YKeNJfNQOfUpdO7bV79650/Wz4Yb3atHxFNapVVs3ApxTW643b3i8+/oKe+09tVXmirBISEhzyTHg0/T25y8ojs5KSklSlShVNnTr1H2MaNmyokydP2o7PPvvMrr1Nmzbat2+foqOjtWzZMm3YsEFdunSxtSckJKhBgwYqXry4tm/frrFjxyoiIkIzZ860xWzcuFGtWrVSp06dtHPnToWEhCgkJER79+61xURFRWny5MmaMWOGNm/erFy5cik4OFhXr17N8PMyTQ+HyuVu1Z7//alPvo7RovFd0rV/PqGLrl1P1SthHygh6arebPsfrZjRS1WbjdLlqymSpI/eaSevPO56JewDnY1PVItGT+rT/3bUs22itOvgH3b9jQ5rqpNnLqpK2SJ257O5uOhK8jVN+2z9Pyay/cZ+oaGTv7Z9zp4tmzYvGqyvonf+y28BcF5XrlxW2bJlFdKsucJ790zXXrx4CQ1+e5iKFCmqq8lX9eknc9T99Y769rto5cuXT5L0/epVGjF8qHqF9VGNgKeVej1Vhw//77b3ixj6th5/vKxOnzrl0OcCHKlRo0Zq1KjRHWOsVqu8vb1v2/bLL79o5cqV2rp1q5588klJ0vvvv6/nn39e7733nnx9fTV//nylpKRo1qxZcnV11RNPPKHY2FiNHz/elrROmjRJDRs2VP/+/SVJ77zzjqKjozVlyhTNmDFDhmFo4sSJGjJkiJo2bSpJ+uSTT1S4cGEtXbpULVu2zNDzUhmFQ63+eb9GTFumb9btTtdWulghBVT205vvLtT2/cd16LfTenP0IrlZc+jVRtVtcU9XKalpC3/Qtn2/6dif5/Tfj1Yp/tIVVa1Q1K6/Bs9WUP2ny2vwhCXp7nX5aop6j16k2Us26tS521dLEhKv6tS5S7ajWoViyuvhrnnfxPzLbwFwXjVr1VHP3n1UP+i527Y/3+QFPR34jIoULarSpcuo34DBSkxM1KH/HZQkXb9+Xf8d86769OuvV1u0UokSfipVurSCGz6frq/PFy7QpUuX1K59R4c+Ex5NjqyMJicnKyEhwe5ITk7+V+Ndv369ChUqpLJly6p79+46d+6crS0mJkZeXl62RFSSgoKC5OLios2bN9tiateuLVdXV1tMcHCwDh48qAsXLthigoKC7O4bHBysmJgb/7949OhRxcXF2cV4enoqICDAFpMRpiajZ8+eVVRUlF566SUFBgYqMDBQL730ksaOHaszZ86YOTTcB1bXG4X5qynXbecMw1BKynU941/Kdm7Trl/1coPqyuuRUxaLRa8EV5ebNbs2bPtrTUqhfHk0bWgrdRr6iS5fScmS8YWGBGrt5oM6fvJClvQH4M6upaToy8WLlCdPHj1etqwk6Zf9+3X61Cm5uLjo1eYhql+npt7o2lmHDtlXRo8cPqwPpk/TqNH/lYsLdRbcA4vjjsjISHl6etodkZGR9zzUhg0b6pNPPtGaNWv03//+Vz/88IMaNWqk1NRUSVJcXJwKFSpkd0327NmVL18+xcXF2WIKFy5sF3Pz891ibm2/9brbxWSEadP0W7duVXBwsHLmzKmgoCA9/vjjkqRTp05p8uTJGjNmjFatWmWX1d9OcnJyut8ujLRUWVyyOWzsyBoHj8Xp+MnzeqfXi+o56jMlXUnRm23rqYh3XnkX8LTFtR0wS/P+21EnfojStWupunw1RS3CP9Svv5+1xcwc2VYffvGTduw/rmI++f712HwKeir42Qpq/9acf90XgDv7Yf06DewXrqtXr6hAwYKa8eEs5c174+f4jz9+lyTNmDpF/QYMku9jj+mTObPVuf1r+mb5Knl6eSklJUWD+oerT7/+8vH1tV0DPCgGDx6s8PBwu3NWq/We+7t1+rtSpUqqXLmySpUqpfXr16t+/fr33K9ZTEtGe/XqpVdeeUUzZsxIt7jXMAx169ZNvXr1umuZNzIyUiNGjLA7l63wU8rhUyPLx4ysdf16mlr2/VDTh7fRyQ1jdf16qtZuPqiVP+3Trf+TGN6jibzyuKtR18k6F5+kF+pW1qdRHRXUcaL2HT6hN1rVUZ6cbho7a3WWja3NCwGKv3TltssLAGStp2oE6PMvlyo+/oK+/OJz9e8bpk8/W6z8+fPLSEuTJHXu0k1BDYIlSSPfjVSD/9TW6tUr9cqrLTVpwjj5lSqlJi80NfMx8JBz5NZOVqv1XyWfd1OyZEkVKFBAhw8fVv369eXt7a3Tp0/bxVy/fl3nz5+3rTP19vbWqb+trb75+W4xt7bfPOfj42MX4+/vn+HxmzaXsWvXLvXp0+e2/+VbLBb16dNHsbGxd+1n8ODBunjxot2RvXD1u16HB8POX37X0y3HqHCtfvJr8Laa9pym/J65dPSPG2tf/IoUUPeWddQ14lOt3/I/7fnfnxo98zvt2H9cXVvUliTVfepxBVT208XNE3Vp6yTt+2a4JOnn+QP04cjX7mlcoU2f1mfLt+ja9dSseVAA/yhnzpwqVry4Klfx14h3Rit7tuxa+tUXkqQCBW/srlGy1F9Ld1xdXfVYkaKKO3lSkrR18yZFr1qpapUrqFrlCurSqb0kqW7NpzVtyuT7+zCACf744w+dO3fOlhAGBgYqPj5e27dvt8WsXbtWaWlpCggIsMVs2LBB165ds8VER0erbNmyyps3ry1mzZo1dveKjo5WYGCgJMnPz0/e3t52MQkJCdq8ebMtJiNMq4x6e3try5YtKleu3G3bt2zZkm4Nwu3c7rcNpugfPgmJN7aAKFWsoKpVKKYR05ZJknK63VhYnWYYdvGpqYZc/v8Xmb5RXyhi6jJbm09BTy2b3lOvDZqtrXuOZXostaqXUelihTRnKS8uAWZIM9KUknJj7XeFJyrK1dVVx44dVbXqN5ZtXbt2TSdO/CkfH19J0riJ7+tq8l/byOzbu0fDh7yl2Z/MV5Gixe7/A+Ch9CBtep+YmKjDhw/bPh89elSxsbHKly+f8uXLpxEjRqh58+by9vbWkSNHNGDAAJUuXVrBwTdmD8qXL6+GDRvq9ddf14wZM3Tt2jX17NlTLVu2lK/vjZ+b1q1ba8SIEerUqZMGDhyovXv3atKkSZowYYLtvr1791adOnU0btw4NW7cWAsXLtS2bdts2z9ZLBaFhYVp1KhRKlOmjPz8/DR06FD5+voqJCQkw89rWjLar18/denSRdu3b1f9+vVtieepU6e0Zs0affjhh3rvvffMGh6ySC53V5Uq+te+oSUey6/Kjz+mCwmX9XvcBTULqqozFxL1e9x5VSzjq/f6v6xv1+/Wmk0HJN1YV3r4+GlNGdJKg8cv0bmLSXqxXmXVf7qsmvWeIUn6Pc7+BaPEyzfWEP/6+xn9eTredr5cSW+5Zs+mvJ65lCen1bYP6e7//Wl3ffuQQG3ZfVT7j5zM8u8DcDaXk5J0/Phx2+c///hDB3755cZLHF5e+mjmDNWt9x8VKFhQ8RcuaOFn83X61Ck9F3xjA+/cuXPrlVdbavrU9+Xt7SNfX1/Nmf2xJKnB/8cULWafcMb//5vAfiVLycPD4348JpCltm3bpnr16tk+31xvGhoaqunTp2v37t2aO3eu4uPj5evrqwYNGuidd96xK87Nnz9fPXv2VP369eXi4qLmzZtr8uS/Zgo8PT21evVq9ejRQ9WrV1eBAgU0bNgwu71In3nmGS1YsEBDhgzRW2+9pTJlymjp0qWqWLGiLWbAgAFKSkpSly5dFB8fr5o1a2rlypVyc3PL8PNaDONvJaf7aNGiRZowYYK2b99uewMsW7Zsql69usLDw/Xqq6/eU7/uVdPvZQdz1KpeRqs/6p3u/LxvNqnL8E/1Rqs66tMuSIXy51Hc2QTNX7ZZkTNX2k2PlypWUKPebKpA/5LKndOqI7+f0cRP1uiz5Vtve89iPvl0cMVIBbSItEs0DywfoeK++dPF3/q/F4/cbjq6erT6jf1Cs5ds/DePDge5sHWK2UNAJmzdslmdO7RLd/7Fpi9pyPARGjSgr/bs3qX4Cxfk5eWlJypW0utdu6tipb/+QMa1a9c0eeJ4Lfv2ayVfvapKlauo/6C3VLp0mTve88eYrSSjDxk3E3c/L93vO4f1ffi9O+8Z6uxMTUZvunbtms6evfFmdIECBZQjR45/1R/JKPDoIhkFHl0ko87pgfgLTDly5LB7CwsAAOB+epDWjDqbByIZBQAAMBO5qHn4MxUAAAAwDZVRAADg9JimNw+VUQAAAJiGyigAAHB6FEbNQ2UUAAAApqEyCgAAnJ6LC6VRs1AZBQAAgGmojAIAAKfHmlHzkIwCAACnx9ZO5mGaHgAAAKahMgoAAJwehVHzUBkFAACAaaiMAgAAp8eaUfNQGQUAAIBpqIwCAACnR2XUPFRGAQAAYBoqowAAwOlRGDUPySgAAHB6TNObh2l6AAAAmIbKKAAAcHoURs1DZRQAAACmoTIKAACcHmtGzUNlFAAAAKahMgoAAJwehVHzUBkFAACAaaiMAgAAp8eaUfNQGQUAAIBpqIwCAACnR2HUPCSjAADA6TFNbx6m6QEAAGAaKqMAAMDpURg1D5VRAAAAmIbKKAAAcHqsGTUPlVEAAACYhsooAABwehRGzUNlFAAAAKahMgoAAJwea0bNQzIKAACcHrmoeZimBwAAgGmojAIAAKfHNL15qIwCAADANFRGAQCA06Myah4qowAAADANlVEAAOD0KIyah8ooAAAATEMyCgAAnJ7FYnHYkVkbNmzQCy+8IF9fX1ksFi1dutTWdu3aNQ0cOFCVKlVSrly55Ovrq3bt2unEiRN2fZQoUSLdOMaMGWMXs3v3btWqVUtubm4qWrSooqKi0o1l8eLFKleunNzc3FSpUiWtWLHCrt0wDA0bNkw+Pj5yd3dXUFCQDh06lKnnJRkFAABOz2Jx3JFZSUlJqlKliqZOnZqu7fLly9qxY4eGDh2qHTt26KuvvtLBgwf14osvposdOXKkTp48aTt69epla0tISFCDBg1UvHhxbd++XWPHjlVERIRmzpxpi9m4caNatWqlTp06aefOnQoJCVFISIj27t1ri4mKitLkyZM1Y8YMbd68Wbly5VJwcLCuXr2a4ee1GIZhZDj6IeFetafZQwDgIBe2TjF7CAAcxM3EN1nqTdrosL7X9X7mnq+1WCxasmSJQkJC/jFm69atqlGjhn777TcVK1ZM0o3KaFhYmMLCwm57zfTp0/X2228rLi5Orq6ukqRBgwZp6dKlOnDggCSpRYsWSkpK0rJly2zXPf300/L399eMGTNkGIZ8fX3Vt29f9evXT5J08eJFFS5cWHPmzFHLli0z9IxURgEAgNNz5DR9cnKyEhIS7I7k5OQsG/vFixdlsVjk5eVld37MmDHKnz+/qlatqrFjx+r69eu2tpiYGNWuXduWiEpScHCwDh48qAsXLthigoKC7PoMDg5WTEyMJOno0aOKi4uzi/H09FRAQIAtJiNIRgEAABwoMjJSnp6edkdkZGSW9H316lUNHDhQrVq1koeHh+38m2++qYULF2rdunXq2rWrRo8erQEDBtja4+LiVLhwYbu+bn6Oi4u7Y8yt7bded7uYjGBrJwAA4PQcubXT4MGDFR4ebnfOarX+636vXbumV199VYZhaPr06XZtt96vcuXKcnV1VdeuXRUZGZkl985KVEYBAAAcyGq1ysPDw+74twnhzUT0t99+U3R0tF1V9HYCAgJ0/fp1HTt2TJLk7e2tU6dO2cXc/Ozt7X3HmFvbb73udjEZQTIKAACcnovF4rAjq91MRA8dOqTvv/9e+fPnv+s1sbGxcnFxUaFChSRJgYGB2rBhg65du2aLiY6OVtmyZZU3b15bzJo1a+z6iY6OVmBgoCTJz89P3t7edjEJCQnavHmzLSYjmKYHAAB4gCQmJurw4cO2z0ePHlVsbKzy5csnHx8fvfzyy9qxY4eWLVum1NRU2/rMfPnyydXVVTExMdq8ebPq1aunPHnyKCYmRn369FHbtm1tiWbr1q01YsQIderUSQMHDtTevXs1adIkTZgwwXbf3r17q06dOho3bpwaN26shQsXatu2bbbtnywWi8LCwjRq1CiVKVNGfn5+Gjp0qHx9fe/49v/fsbUTgIcKWzsBjy4zt3ZqMHWTw/pe3ePpTMWvX79e9erVS3c+NDRUERER8vPzu+1169atU926dbVjxw698cYbOnDggJKTk+Xn56fXXntN4eHhdssDdu/erR49emjr1q0qUKCAevXqpYEDB9r1uXjxYg0ZMkTHjh1TmTJlFBUVpeeff97WbhiGhg8frpkzZyo+Pl41a9bUtGnT9Pjjj2f4eUlGATxUSEaBR5eZyWjwtM0O63vVGwEO6/tRwJpRAAAAmIY1owAAwOm5OHBrJ9wZlVEAAACYhsooAABwehZH7nqPO6IyCgAAANNQGQUAAE6Pwqh5qIwCAADANFRGAQCA07OI0qhZSEYBAIDTY2sn8zBNDwAAANNQGQUAAE6PrZ3MQ2UUAAAApqEyCgAAnB6FUfNQGQUAAIBpqIwCAACn50Jp1DRURgEAAGAaKqMAAMDpURg1D8koAABwemztZJ4MJaO7d+/OcIeVK1e+58EAAADAuWQoGfX395fFYpFhGLdtv9lmsViUmpqapQMEAABwNAqj5slQMnr06FFHjwMAAABOKEPJaPHixR09DgAAANOwtZN57mlrp3nz5unZZ5+Vr6+vfvvtN0nSxIkT9fXXX2fp4AAAAPBoy3QyOn36dIWHh+v5559XfHy8bY2ol5eXJk6cmNXjAwAAcDiLAw/cWaaT0ffff18ffvih3n77bWXLls12/sknn9SePXuydHAAAAB4tGV6n9GjR4+qatWq6c5brVYlJSVlyaAAAADuJ/YZNU+mK6N+fn6KjY1Nd37lypUqX758VowJAADgvnKxOO7AnWW6MhoeHq4ePXro6tWrMgxDW7Zs0WeffabIyEh99NFHjhgjAAAAHlGZTkY7d+4sd3d3DRkyRJcvX1br1q3l6+urSZMmqWXLlo4YIwAAgEMxTW+ee/rb9G3atFGbNm10+fJlJSYmqlChQlk9LgAAADiBe0pGJen06dM6ePCgpBu/TRQsWDDLBgUAAHA/URg1T6ZfYLp06ZJee+01+fr6qk6dOqpTp458fX3Vtm1bXbx40RFjBAAAwCMq08lo586dtXnzZi1fvlzx8fGKj4/XsmXLtG3bNnXt2tURYwQAAHAoi8XisAN3lulp+mXLlmnVqlWqWbOm7VxwcLA+/PBDNWzYMEsHBwAAgEdbppPR/Pnzy9PTM915T09P5c2bN0sGBQAAcD+xH6h5Mj1NP2TIEIWHhysuLs52Li4uTv3799fQoUOzdHAAAAD3A9P05slQZbRq1ap2X+ahQ4dUrFgxFStWTJJ0/PhxWa1WnTlzhnWjAAAAyLAMJaMhISEOHgYAAIB5qF+aJ0PJ6PDhwx09DgAAADihe970HgAA4FHhwtpO02Q6GU1NTdWECRP0+eef6/jx40pJSbFrP3/+fJYNDgAAAI+2TL9NP2LECI0fP14tWrTQxYsXFR4ermbNmsnFxUUREREOGCIAAIBjWSyOO3BnmU5G58+frw8//FB9+/ZV9uzZ1apVK3300UcaNmyYNm3a5IgxAgAA4BGV6WQ0Li5OlSpVkiTlzp3b9vfomzRpouXLl2ft6AAAAO4D9hk1T6aT0SJFiujkyZOSpFKlSmn16tWSpK1bt8pqtWbt6AAAAPBIy3Qy+tJLL2nNmjWSpF69emno0KEqU6aM2rVrp44dO2b5AAEAAByNNaPmyfTb9GPGjLH9c4sWLVS8eHFt3LhRZcqU0QsvvJClgwMAALgf2NrJPJmujP7d008/rfDwcAUEBGj06NFZMSYAAAA4iX+djN508uRJDR06NKu6AwAAuG8epGn6DRs26IUXXpCvr68sFouWLl1q124YhoYNGyYfHx+5u7srKChIhw4dsos5f/682rRpIw8PD3l5ealTp05KTEy0i9m9e7dq1aolNzc3FS1aVFFRUenGsnjxYpUrV05ubm6qVKmSVqxYkemx3E2WJaMAAAD495KSklSlShVNnTr1tu1RUVGaPHmyZsyYoc2bNytXrlwKDg7W1atXbTFt2rTRvn37FB0drWXLlmnDhg3q0qWLrT0hIUENGjRQ8eLFtX37do0dO1YRERGaOXOmLWbjxo1q1aqVOnXqpJ07dyokJEQhISHau3dvpsZyNxbDMIzMfEH/ZNeuXapWrZpSU1Ozort/xb1qT7OHAMBBLmydYvYQADiIm4l/pLzHkl8c1vf450sqOTnZ7pzVas3QLkQWi0VLlixRSEiIpBuVSF9fX/Xt21f9+vWTJF28eFGFCxfWnDlz1LJlS/3yyy+qUKGCtm7dqieffFKStHLlSj3//PP6448/5Ovrq+nTp+vtt99WXFycXF1dJUmDBg3S0qVLdeDAAUk33g1KSkrSsmXLbON5+umn5e/vrxkzZmRoLBlBZRQAAMCBIiMj5enpaXdERkbeU19Hjx5VXFycgoKCbOc8PT0VEBCgmJgYSVJMTIy8vLxsiagkBQUFycXFRZs3b7bF1K5d25aISlJwcLAOHjyoCxcu2GJuvc/NmJv3ychYMiLDv4OEh4ffsf3MmTMZvqmjHf1hgtlDAOAgWTOXAwD2HFmdGzx4cLo86l73Zo+Li5MkFS5c2O584cKFbW1xcXEqVKiQXXv27NmVL18+uxg/P790fdxsy5s3r+Li4u56n7uNJSMynIzu3LnzrjG1a9fO8I0BAACcQUan5J1VhpPRdevWOXIcAAAApnlY/mynt7e3JOnUqVPy8fGxnT916pT8/f1tMadPn7a77vr16zp//rztem9vb506dcou5ubnu8Xc2n63sWQEa0YBAIDTc7E47shKfn5+8vb2tv01TOnGm/GbN29WYGCgJCkwMFDx8fHavn27LWbt2rVKS0tTQECALWbDhg26du2aLSY6Olply5ZV3rx5bTG33udmzM37ZGQsGUEyCgAA8ABJTExUbGysYmNjJd14USg2NlbHjx+XxWJRWFiYRo0apW+++UZ79uxRu3bt5Ovra3vjvnz58mrYsKFef/11bdmyRT///LN69uypli1bytfXV5LUunVrubq6qlOnTtq3b58WLVqkSZMm2a1t7d27t1auXKlx48bpwIEDioiI0LZt29Sz541dizIylowwcRMFAACAB0NWVzD/jW3btqlevXq2zzcTxNDQUM2ZM0cDBgxQUlKSunTpovj4eNWsWVMrV66Um5ub7Zr58+erZ8+eql+/vlxcXNS8eXNNnjzZ1u7p6anVq1erR48eql69ugoUKKBhw4bZ7UX6zDPPaMGCBRoyZIjeeustlSlTRkuXLlXFihVtMRkZy91k2T6jD5K4hGt3DwLwUPJ0z2H2EAA4iJk/3uHfHHBY3+NfLOewvh8FVEYBAIDTe1heYHoU3dOa0R9//FFt27ZVYGCg/vzzT0nSvHnz9NNPP2Xp4AAAAPBoy3Qy+uWXXyo4OFju7u7auXOn7c9bXbx4UaNHj87yAQIAADjaw/I2/aMo08noqFGjNGPGDH344YfKkeOvxR3PPvusduzYkaWDAwAAwKMt02tGDx48eNu/tOTp6an4+PisGBMAAMB9xZJR82S6Murt7a3Dhw+nO//TTz+pZMmSWTIoAACA+8nFYnHYgTvLdDL6+uuvq3fv3tq8ebMsFotOnDih+fPnq1+/furevbsjxggAAIBHVKan6QcNGqS0tDTVr19fly9fVu3atWW1WtWvXz/16tXLEWMEAABwKP4kpXnuedP7lJQUHT58WImJiapQoYJy586d1WO7Z2x6Dzy62PQeeHSZ+eP91or/Oazv0c8/7rC+HwX3vOm9q6urKlSokJVjAQAAMAVLO82T6WS0Xr16d/wrBWvXrv1XAwIAAIDzyHQy6u/vb/f52rVrio2N1d69exUaGppV4wIAALhveOvdPJlORidMmHDb8xEREUpMTPzXAwIAAIDzyLKXx9q2batZs2ZlVXcAAAD3jcXiuAN3ds8vMP1dTEyM3Nzcsqo7AACA+4a/IW+eTCejzZo1s/tsGIZOnjypbdu2aejQoVk2MAAAADz6Mp2Menp62n12cXFR2bJlNXLkSDVo0CDLBgYAAHC/8AKTeTKVjKampqpDhw6qVKmS8ubN66gxAQAAwElk6gWmbNmyqUGDBoqPj3fQcAAAAO4/XmAyT6bfpq9YsaJ+/fVXR4wFAAAATibTyeioUaPUr18/LVu2TCdPnlRCQoLdAQAA8LBxsTjuwJ1leM3oyJEj1bdvXz3//POSpBdffNHuz4IahiGLxaLU1NSsHyUAAAAeSRlORkeMGKFu3bpp3bp1jhwPAADAfWcRJUyzZDgZNQxDklSnTh2HDQYAAMAMTKebJ1NrRi28EgYAAIAslKl9Rh9//PG7JqTnz5//VwMCAAC436iMmidTyeiIESPS/QUmAAAA4F5lKhlt2bKlChUq5KixAAAAmIKliObJ8JpR/ksCAABAVsv02/QAAACPGtaMmifDyWhaWpojxwEAAAAnlKk1owAAAI8iViOah2QUAAA4PReyUdNkatN7AAAAICtRGQUAAE6PF5jMQ2UUAAAApqEyCgAAnB5LRs1DZRQAAACmoTIKAACcnosojZqFyigAAABMQ2UUAAA4PdaMmodkFAAAOD22djIP0/QAAAAwDZVRAADg9PhzoOahMgoAAADTUBkFAABOj8KoeaiMAgAAwDQkowAAwOm5WCwOOzKjRIkSslgs6Y4ePXpIkurWrZuurVu3bnZ9HD9+XI0bN1bOnDlVqFAh9e/fX9evX7eLWb9+vapVqyar1arSpUtrzpw56cYydepUlShRQm5ubgoICNCWLVsy96VmEMkoAADAA2Lr1q06efKk7YiOjpYkvfLKK7aY119/3S4mKirK1paamqrGjRsrJSVFGzdu1Ny5czVnzhwNGzbMFnP06FE1btxY9erVU2xsrMLCwtS5c2etWrXKFrNo0SKFh4dr+PDh2rFjh6pUqaLg4GCdPn06y5/ZYhiGkeW9miwu4ZrZQwDgIJ7uOcweAgAHMfPHe9bW4w7ru+NTxe752rCwMC1btkyHDh2SxWJR3bp15e/vr4kTJ942/rvvvlOTJk104sQJFS5cWJI0Y8YMDRw4UGfOnJGrq6sGDhyo5cuXa+/evbbrWrZsqfj4eK1cuVKSFBAQoKeeekpTpkyRJKWlpalo0aLq1auXBg0adM/PcztURgEAgNNzceCRnJyshIQEuyM5OfmuY0pJSdGnn36qjh07ynLLdP/8+fNVoEABVaxYUYMHD9bly5dtbTExMapUqZItEZWk4OBgJSQkaN++fbaYoKAgu3sFBwcrJibGdt/t27fbxbi4uCgoKMgWk5VIRgEAABwoMjJSnp6edkdkZORdr1u6dKni4+PVvn1727nWrVvr008/1bp16zR48GDNmzdPbdu2tbXHxcXZJaKSbJ/j4uLuGJOQkKArV67o7NmzSk1NvW3MzT6yEls7AQAAp2fJ5ItGmTF48GCFh4fbnbNarXe97uOPP1ajRo3k6+trO9elSxfbP1eqVEk+Pj6qX7++jhw5olKlSmXdoO8jklEAAAAHslqtGUo+b/Xbb7/p+++/11dffXXHuICAAEnS4cOHVapUKXl7e6d76/3UqVOSJG9vb9t/3jx3a4yHh4fc3d2VLVs2ZcuW7bYxN/vISkzTAwAAp2dx4HEvZs+erUKFCqlx48Z3jIuNjZUk+fj4SJICAwO1Z88eu7feo6Oj5eHhoQoVKthi1qxZY9dPdHS0AgMDJUmurq6qXr26XUxaWprWrFlji8lKJKMAAAAPkLS0NM2ePVuhoaHKnv2vSewjR47onXfe0fbt23Xs2DF98803ateunWrXrq3KlStLkho0aKAKFSrotdde065du7Rq1SoNGTJEPXr0sFVnu3Xrpl9//VUDBgzQgQMHNG3aNH3++efq06eP7V7h4eH68MMPNXfuXP3yyy/q3r27kpKS1KFDhyx/XqbpAQCA08vs5vSO9P333+v48ePq2LGj3XlXV1d9//33mjhxopKSklS0aFE1b95cQ4YMscVky5ZNy5YtU/fu3RUYGKhcuXIpNDRUI0eOtMX4+flp+fLl6tOnjyZNmqQiRYroo48+UnBwsC2mRYsWOnPmjIYNG6a4uDj5+/tr5cqV6V5qygrsMwrgocI+o8Cjy8wf70+3/+GwvttWL+Kwvh8FVEYBAIDTe3Dqos6HZBQAADi9B2iW3unwAhMAAABMQ2UUAAA4PUdueo87ozIKAAAA01AZBQAATo/qnHn47gEAAGAaKqMAAMDpsWbUPFRGAQAAYBoqowAAwOlRFzUPlVEAAACYhsooAABweqwZNQ/JKAAAcHpMFZuH7x4AAACmoTIKAACcHtP05qEyCgAAANNQGQUAAE6Puqh5qIwCAADANFRGAQCA02PJqHmojAIAAMA0VEYBAIDTc2HVqGlIRgEAgNNjmt48TNMDAADANFRGAQCA07MwTW8aKqMAAAAwDZVRAADg9Fgzah4qowAAADANlVEAAOD02NrJPFRGAQAAYBoqowAAwOmxZtQ8JKMAAMDpkYyah2l6AAAAmIbKKAAAcHpsem8eKqMAAAAwDZVRAADg9FwojJqGyigAAABMQ2UUAAA4PdaMmofKKAAAAExDZRQAADg99hk1D8koAABwekzTm4dpegAAAJiGyigAAHB6bO1kHiqjAAAAMA2VUQAA4PRYM2oeKqMAAAAwDZVR3He7dmzTZ/Nm638H9uvc2TMaNXaSatWtb2uv81TF217X7c1wtXqtoyQp4eJFTRo7Wht/Wi8Xi4tq/ydIvfoOVs6cOW3xW2J+1uyZU3X018NydbWqStXqeiOsv3x8H5MknTt7RlMnjtXBX/bpz9+Pq3mLNurVd5ADnxxwPh9/+IHWfL9ax47+Kqubm6r4V1VYn34q4VfSFnP27BlNeC9Km2I2KulykkqU8FPnLt0U9Fxwuv5SUlLUttUr+t/BA1r4xVKVK1dekjR96vv6YPqUdPFu7u7atDXWYc+HRwdbO5mHyijuuytXrqj042UVNuDt27Z/9d16u2Pg0HdksVhUp95ztph3hg7UsV8Pa9yUDxU5Yap27dyu90ZH2NpP/vmH3u7XS1WfrKGP53+h997/QBfj4zV0QJgtJiUlRV5eefVaxy4qVaasox4XcGrbt21Ri1Zt9MmCzzVj5mxdv3Zd3bt00pXLl20xQwYP1LFjRzVxynR98dW3qh/0nAb0DdOBX/an62/CuCgVLFQo3fnQDh31/fqf7I6SpUrruQYNHfp8AP49klHcd08/W0udu7+p2vWCbtuev0ABu+PnDetUtXoN+RYpKkk6dvSItsT8pP5DRqhCxcqq7F9Nvfu9pbWrv9PZM6clSQcP7Fdqapo6d39TjxUppsfLVVCLtu11+H8HdP36NUmSj+9jerPfYDVs3FS5c+e+Pw8POJlpH3yspiHNVLp0GZUtV04j3x2jkydPaP/+fbaYXbE71ap1W1WqVFlFihbV613fUJ48Htq/b59dXz/9+IM2bfxZ4f0GprtPzpy5VKBAQdtx7uw5/XrksF5q9rLDnxGPBosDD9wZySgeaOfPnVXMTxv0fNNmtnP79uxS7jweKlfhr+n86jWelouLi/bv3S1JKluuglxcLPru2yVKTU1VYuIlrf7uW1Wv8bSyZ89x358DwA2JiZckSZ6enrZzVfyratXK73TxYrzS0tK0csVyJack68kaNWwx586e1ciIoRoVGSU3N7e73mfJV4tVvEQJVav+ZNY/BB5JLhaLw47MiIiIkMVisTvKlStna7969ap69Oih/PnzK3fu3GrevLlOnTpl18fx48fVuHFj5cyZU4UKFVL//v11/fp1u5j169erWrVqslqtKl26tObMmZNuLFOnTlWJEiXk5uamgIAAbdmyJVPPklEPdDL6+++/q2PHjneMSU5OVkJCgt2RnJx8n0YIR1u5/BvlzJXTrop6/txZ5c2bzy4ue/bsyuPhqfPnzkqSfB4rovfen6kPp03Sc89WU+N6gTpzKk4RkePu6/gB/CUtLU1jx4yWf9VqKl3mcdv5qHETdf36ddV5NkA1qlXSqJHDNH7iFBUrVlySZBiGhg0ZpFdebaknKla6632Sk5O1Ytm3CqEqiofUE088oZMnT9qOn376ydbWp08fffvtt1q8eLF++OEHnThxQs2a/VWwSU1NVePGjZWSkqKNGzdq7ty5mjNnjoYNG2aLOXr0qBo3bqx69eopNjZWYWFh6ty5s1atWmWLWbRokcLDwzV8+HDt2LFDVapUUXBwsE6fPp3lz/tAJ6Pnz5/X3Llz7xgTGRkpT09Pu+P98f+9TyOEo333zRIFNWwiq9WaqevOnT2rsaMjFNy4qWbMXajJH8xRjhw5NHxguAzDcNBoAdxJ5KgROnz4kP47doLd+WlTJunSpQR98NEczV/4pdq266AB/cJ06H8HJUmfzZ+npKQkdezcNUP3WbsmWpcvJ+nFF1/K8mfAo+tBmqbPnj27vL29bUeBAgUkSRcvXtTHH3+s8ePH6z//+Y+qV6+u2bNna+PGjdq0aZMkafXq1dq/f78+/fRT+fv7q1GjRnrnnXc0depUpaSkSJJmzJghPz8/jRs3TuXLl1fPnj318ssva8KEv342x48fr9dff10dOnRQhQoVNGPGDOXMmVOzZs26hye6y/NmeY+Z8M0339yx/ddff71rH4MHD1Z4eLjduQvJD3SOjQzatXO7jv92VMNHj7U7ny9/AV24cN7u3PXr13Up4aLy5b/xA7t08WfKlSu3ur/Z1xbz9sgxeqVJkPbv3a0nKlVx/AMAsIl8d6Q2/LBes+Z+qsLe3rbzvx8/roULPtUXS5epdOkykqSy5cpp545tWvTZfA0ZPlJbtmzS7l2xqlHNvirapkVzNWr8gkaNti9ALPlysWrVrqv8//9/4IDZkpOT083aWq3Wfyy0HDp0SL6+vnJzc1NgYKAiIyNVrFgxbd++XdeuXVNQ0F+zheXKlVOxYsUUExOjp59+WjExMapUqZIKFy5siwkODlb37t21b98+Va1aVTExMXZ93IwJCwuTdOMF3+3bt2vw4MG2dhcXFwUFBSkmJubffh3pmJqMhoSEyGKx3LFSZbnLWovb/Zd5OeFalowP5lrx9VcqW76CSj9ezu78E5WqKPFSgg7+sk9lyz8hSdq5bbPS0tJUoWJlSTfW1Li42P9Ski1bNkmSkZZ2H0YPQLoxxT5m9DtauyZaH82ep8f+/0XEm65evSJJcrHY/7y6uGRT2v//f8PAwUPUs1eYre306dN6o2sn/fe9Car0t18s//zjd23dslmT3p/ugKfBI82BbxpFRkZqxIgRdueGDx+uiIiIdLEBAQGaM2eOypYtq5MnT2rEiBGqVauW9u7dq7i4OLm6usrLy8vumsKFCysuLk6SFBcXZ5eI3my/2XanmISEBF25ckUXLlxQamrqbWMOHDiQ6ee/G1OTUR8fH02bNk1Nmza9bXtsbKyqV69+n0cFR7t8+bL+/P247fPJE3/q0MED8vD0VGFvH0lSUmKi1q9ZrTfC+qW7voRfKdUIrKmx70ao7+Bhun79miaOHa3/NGikAgVvbPkSWLO2Fn/2ieZ8OF1Bwc/r8uUkfTh1krx9fFWmbHlbX4cO3vihunLlsuIvXNChgweUI0cOlShZypFfAeA0Ro8aoe9WLNPEydOUK1cunT17RpKUO3ceubm5qYRfSRUtVlyjRg5Tn34D5eXppXVrv9emmJ81eeoHkiQfH1+7Pt3/fz/hIkWL2VVZJWnpki9VoGBBPVur9n14OiBjbjeL+09V0UaNGtn+uXLlygoICFDx4sX1+eefy93d3aHjNIupyWj16tW1ffv2f0xG71Y1xcPp4C97FdbtrxfTpk6IkiQ1bNxUgyPelSStWf2dDMNQ/eDnb9vH0Hf+q4lj31WfNzrZNr1/s99btvZqTwVo6Kj/6rNPZmvhvFmyurnriUpVFDV5hqy3vInbue1fLzgc/GW/vl+1XN4+vlr0zeosfWbAWS1e9JkkqXOH1+zOjxgVqaYhzZQjRw5NmT5TkyeMU+8e3XT5ymUVK1pM77w7RrVq18nUvdLS0vTN0iV6sWkz20wIkFGO/HOgd5qSvxsvLy89/vjjOnz4sJ577jmlpKQoPj7erjp66tQpef//L2be3t7p3nq/+bb9rTF/fwP/1KlT8vDwkLu7u7Jly6Zs2bLdNsb7b78AZgWLYWK29+OPPyopKUkNG95+U+KkpCRt27ZNdepk7l9IcUzTA48sT3e25gIeVWb+eG8+ctFhfQeU8rx70D9ITExUsWLFFBERodDQUBUsWFCfffaZmjdvLkk6ePCgypUrZ1sz+t1336lJkyY6efKkCv3/H4iYOXOm+vfvr9OnT8tqtWrgwIFasWKF9uzZY7tP69atdf78ea1cufLGmAMCVKNGDb3//vuSbvyyV6xYMfXs2VODBmXtXys0NRl1FJJR4NFFMgo8usz88d7yq+OS0RolM56M9uvXTy+88IKKFy+uEydOaPjw4YqNjdX+/ftVsGBBde/eXStWrNCcOXPk4eGhXr16SZI2btwo6cbWTv7+/vL19VVUVJTi4uL02muvqXPnzho9erSkG1s7VaxYUT169FDHjh21du1avfnmm1q+fLmCg2/8Gd5FixYpNDRUH3zwgWrUqKGJEyfq888/14EDB9KtJf23+Nv0AADA6T0ofynpjz/+UKtWrXTu3DkVLFhQNWvW1KZNm1SwYEFJ0oQJE+Ti4qLmzZsrOTlZwcHBmjZtmu36bNmyadmyZerevbsCAwOVK1cuhYaGauTIkbYYPz8/LV++XH369NGkSZNUpEgRffTRR7ZEVJJatGihM2fOaNiwYYqLi5O/v79WrlyZ5YmoRGUUwEOGyijw6DLzx3urAyujT2WiMuqMqIwCAAA8KKVRJ8Tu8AAAADANlVEAAOD0HLm1E+6MyigAAABMQ2UUAAA4vbv89XE4EJVRAAAAmIbKKAAAcHoURs1DMgoAAEA2ahqm6QEAAGAaKqMAAMDpsbWTeaiMAgAAwDRURgEAgNNjayfzUBkFAACAaaiMAgAAp0dh1DxURgEAAGAaKqMAAACURk1DMgoAAJweWzuZh2l6AAAAmIbKKAAAcHps7WQeKqMAAAAwDZVRAADg9CiMmofKKAAAAExDZRQAAIDSqGmojAIAAMA0VEYBAIDTY59R81AZBQAAgGmojAIAAKfHPqPmIRkFAABOj1zUPEzTAwAAwDRURgEAACiNmobKKAAAAExDZRQAADg9tnYyD5VRAAAAmIbKKAAAcHps7WQeKqMAAAAwDZVRAADg9CiMmodkFAAAgGzUNEzTAwAAwDRURgEAgNNjayfzUBkFAACAaaiMAgAAp8fWTuahMgoAAADTUBkFAABOj8KoeaiMAgAAwDRURgEAACiNmoZkFAAAOD22djIP0/QAAAAwDckoAABwehaL447MiIyM1FNPPaU8efKoUKFCCgkJ0cGDB+1i6tatK4vFYnd069bNLub48eNq3LixcubMqUKFCql///66fv26Xcz69etVrVo1Wa1WlS5dWnPmzEk3nqlTp6pEiRJyc3NTQECAtmzZkrkHygCSUQAAgAfEDz/8oB49emjTpk2Kjo7WtWvX1KBBAyUlJdnFvf766zp58qTtiIqKsrWlpqaqcePGSklJ0caNGzV37lzNmTNHw4YNs8UcPXpUjRs3Vr169RQbG6uwsDB17txZq1atssUsWrRI4eHhGj58uHbs2KEqVaooODhYp0+fztJnthiGYWRpjw+AuIRrZg8BgIN4uucwewgAHMTMH+9jZ686rO8SBdzu+dozZ86oUKFC+uGHH1S7dm1JNyqj/v7+mjhx4m2v+e6779SkSROdOHFChQsXliTNmDFDAwcO1JkzZ+Tq6qqBAwdq+fLl2rt3r+26li1bKj4+XitXrpQkBQQE6KmnntKUKVMkSWlpaSpatKh69eqlQYMG3fMz/R2VUQAAAAdKTk5WQkKC3ZGcnJyhay9evChJypcvn935+fPnq0CBAqpYsaIGDx6sy5cv29piYmJUqVIlWyIqScHBwUpISNC+fftsMUFBQXZ9BgcHKyYmRpKUkpKi7du328W4uLgoKCjIFpNVSEYBAAAsjjsiIyPl6elpd0RGRt51SGlpaQoLC9Ozzz6rihUr2s63bt1an376qdatW6fBgwdr3rx5atu2ra09Li7OLhGVZPscFxd3x5iEhARduXJFZ8+eVWpq6m1jbvaRVdjaCQAAwIEGDx6s8PBwu3NWq/Wu1/Xo0UN79+7VTz/9ZHe+S5cutn+uVKmSfHx8VL9+fR05ckSlSpXKmkHfRySjAADA6Tlyn1Gr1Zqh5PNWPXv21LJly7RhwwYVKVLkjrEBAQGSpMOHD6tUqVLy9vZO99b7qVOnJEne3t62/7x57tYYDw8Pubu7K1u2bMqWLdttY272kVWYpgcAAE7vQdnayTAM9ezZU0uWLNHatWvl5+d312tiY2MlST4+PpKkwMBA7dmzx+6t9+joaHl4eKhChQq2mDVr1tj1Ex0drcDAQEmSq6urqlevbheTlpamNWvW2GKyCpVRAACAB0SPHj20YMECff3118qTJ49tfaanp6fc3d115MgRLViwQM8//7zy58+v3bt3q0+fPqpdu7YqV64sSWrQoIEqVKig1157TVFRUYqLi9OQIUPUo0cPW4W2W7dumjJligYMGKCOHTtq7dq1+vzzz7V8+XLbWMLDwxUaGqonn3xSNWrU0MSJE5WUlKQOHTpk6TOztROAhwpbOwGPLjN/vH8/n7G32+9F0XwZn6K3/EMpdfbs2Wrfvr1+//13tW3bVnv37lVSUpKKFi2ql156SUOGDJGHh4ct/rffflP37t21fv165cqVS6GhoRozZoyyZ/+rDrl+/Xr16dNH+/fvV5EiRTR06FC1b9/e7r5TpkzR2LFjFRcXJ39/f02ePNm2LCCrkIwCeKiQjAKPLpJR58Q0PQAAcHqZXduJrMMLTAAAADANlVEAAAAHbu2EO6MyCgAAANNQGQUAAE6PNaPmIRkFAABOj1zUPEzTAwAAwDRURgEAgNNjmt48VEYBAABgGiqjAADA6VlYNWoaKqMAAAAwDZVRAAAACqOmoTIKAAAA01AZBQAATo/CqHlIRgEAgNNjayfzME0PAAAA01AZBQAATo+tncxDZRQAAACmoTIKAABAYdQ0VEYBAABgGiqjAADA6VEYNQ+VUQAAAJiGyigAAHB67DNqHpJRAADg9NjayTxM0wMAAMA0VEYBAIDTY5rePFRGAQAAYBqSUQAAAJiGZBQAAACmYc0oAABweqwZNQ+VUQAAAJiGyigAAHB67DNqHpJRAADg9JimNw/T9AAAADANlVEAAOD0KIyah8ooAAAATENlFAAAgNKoaaiMAgAAwDRURgEAgNNjayfzUBkFAACAaaiMAgAAp8c+o+ahMgoAAADTUBkFAABOj8KoeUhGAQAAyEZNwzQ9AAAATENlFAAAOD22djIPlVEAAACYhsooAABwemztZB4qowAAADCNxTAMw+xBAPcqOTlZkZGRGjx4sKxWq9nDAZCF+PkGnAPJKB5qCQkJ8vT01MWLF+Xh4WH2cABkIX6+AefAND0AAABMQzIKAAAA05CMAgAAwDQko3ioWa1WDR8+nJcbgEcQP9+Ac+AFJgAAAJiGyigAAABMQzIKAAAA05CMAgAAwDQkowAAADANySgealOnTlWJEiXk5uamgIAAbdmyxewhAfiXNmzYoBdeeEG+vr6yWCxaunSp2UMC4EAko3hoLVq0SOHh4Ro+fLh27NihKlWqKDg4WKdPnzZ7aAD+haSkJFWpUkVTp041eygA7gO2dsJDKyAgQE899ZSmTJkiSUpLS1PRokXVq1cvDRo0yOTRAcgKFotFS5YsUUhIiNlDAeAgVEbxUEpJSdH27dsVFBRkO+fi4qKgoCDFxMSYODIAAJAZJKN4KJ09e1apqakqXLiw3fnChQsrLi7OpFEBAIDMIhkFAACAaUhG8VAqUKCAsmXLplOnTtmdP3XqlLy9vU0aFQAAyCySUTyUXF1dVb16da1Zs8Z2Li0tTWvWrFFgYKCJIwMAAJmR3ewBAPcqPDxcoaGhevLJJ1WjRg1NnDhRSUlJ6tChg9lDA/AvJCYm6vDhw7bPR48eVWxsrPLly6dixYqZODIAjsDWTnioTZkyRWPHjlVcXJz8/f01efJkBQQEmD0sAP/C+vXrVa9evXTnQ0NDNWfOnPs/IAAORTIKAAAA07BmFAAAAKYhGQUAAIBpSEYBAABgGpJRAAAAmIZkFAAAAKYhGQUAAIBpSEYBAABgGpJRAAAAmIZkFECWad++vUJCQmyf69atq7CwsPs+jvXr18tisSg+Pt5h9/j7s96L+zFOAHjQkYwCj7j27dvLYrHIYrHI1dVVpUuX1siRI3X9+nWH3/urr77SO++8k6HY+52YlShRQhMnTrwv9wIA/LPsZg8AgOM1bNhQs2fPVnJyslasWKEePXooR44cGjx4cLrYlJQUubq6Zsl98+XLlyX9AAAeXVRGASdgtVrl7e2t4sWLq3v37goKCtI333wj6a/p5nfffVe+vr4qW7asJOn333/Xq6++Ki8vL+XLl09NmzbVsWPHbH2mpqYqPDxcXl5eyp8/vwYMGCDDMOzu+/dp+uTkZA0cOFBFixaV1WpV6dKl9fHHH+vYsWOqV6+eJClv3ryyWCxq3769JCktLU2RkZHy8/OTu7u7qlSpoi+++MLuPitWrNDjjz8ud3d31atXz26c9yI1NVWdOnWy3bNs2bKaNGnSbWNHjBihggULysPDQ926dVNKSoqtLSNjBwBnR2UUcELu7u46d+6c7fOaNWvk4eGh6OhoSdK1a9cUHByswMBA/fjjj8qePbtGjRqlhg0bavfu3XJ1ddW4ceM0Z84czZo1S+XLl9e4ceO0ZMkS/ec///nH+7Zr104xMTGaPHmyqlSpoqNHj+rs2bMqWrSovvzySzVv3lwHDx6Uh4eH3N3dJUmRkZH69NNPNWPGDJUpU0YbNmxQ27ZtVbBgQdWpU0e///67mjVrph49eqhLly7atm2b+vbt+6++n7S0NBUpUkSLFy9W/vz5tXHjRnXp0kU+Pj569dVX7b43Nzc3rV+/XseOHVOHDh2UP39+vfvuuxkaOwBAkgHgkRYaGmo0bdrUMAzDSEtLM6Kjow2r1Wr069fP1l64cGEjOTnZds28efOMsmXLGmlpabZzycnJhru7u7Fq1SrDMAzDx8fHiIqKsrVfu3bNKFKkiO1ehmEYderUMXr37m0YhmEcPHjQkGRER0ffdpzr1q0zJBkXLlywnbt69aqRM2dOY+PGjXaxnTp1Mlq1amUYhmEMHjzYqFChgl37wIED0/X1d8WLFzcmTJjwj+1/16NHD6N58+a2z6GhoUa+fPmMpKQk27np06cbuXPnNlJTUzM09ts9MwA4GyqjgBNYtmyZcufOrWvXriktLU2tW7dWRESErb1SpUp260R37dqlw4cPK0+ePHb9XL16VUeOHNHFixd18uRJBQQE2NqyZ8+uJ598Mt1U/U2xsbHKli1bpiqChw8f1uXLl/Xcc8/ZnU9JSVHVqlUlSb/88ovdOCQpMDAww/f4J1OnTtWsWbN0/PhxXblyRSkpKfL397eLqVKlinLmzGl338TERP3+++9KTEy869gBAEzTA06hXr16mj59ulxdXeXr66vs2e1/9HPlymX3OTExUdWrV9f8+fPT9VWwYMF7GsPNaffMSExMlCQtX75cjz32mF2b1Wq9p3FkxMKFC9WvXz+NGzdOgYGBypMnj8aOHavNmzdnuA+zxg4ADxuSUcAJ5MqVS6VLl85wfLVq1bRo0SIVKlRIHh4et43x8fHR5s2bVbt2bUnS9evXtX37dlWrVu228ZUqVVJaWpp++OEHBQUFpWu/WZlNTU21natQoYKsVquOHz/+jxXV8uXL217GumnTpk13f8g7+Pnnn/XMM8/ojTfesJ07cuRIurhdu3bpypUrtkR706ZNyp07t4oWLap8+fLddewAAN6mB3Abbdq0UYECBdS0aVP9+OOPOnr0qNavX68333xTf/zxhySpd+/eGjNmjJYuXaoDBw7ojTfeuOMeoSVKlFBoaKg6duyopUuX2vr8/PPPJUnFixeXxWLRsmXLdObMGSUmJipPnjzq16+f+vTpo7lz5+rIkSPasWOH3n//fc2dO1eS1K1bNx06dEj9+/fXwYMHtWDBAs2ZMydDz/nnn38qNjbW7rhw4YLKlCmjbdu2adWqVfrf//6noUOHauvWremuT0lJUadOnbR//36tWLFCw4cPV8+ePeXi4pKhsQMAxAtMwKPu1heYMtN+8uRJo127dkaBAgUMq9VqlCxZ0nj99deNixcvGoZx44Wl3r17Gx4eHoaXl5cRHh5utGvX7h9fYDIMw7hy5YrRp08fw8fHx3B1dTVKly5tzJo1y9Y+cuRIw9vb27BYLEZoaKhhGDdeupo4caJRtmxZI0eOHEbBggWN4OBg44cffrBd9+233xqlS5c2rFarUatWLWPWrFkZeoFJUrpj3rx5xtWrV4327dsbnp6ehpeXl9G9e3dj0KBBRpUqVdJ9b8OGDTPy589v5M6d23j99deNq1ev2mLuNnZeYAIAw7AYxj+8bQAAAAA4GNP0AAAAMA3JKAAAAExDMgoAAADTkIwCAADANCSjAAAAMA3JKAAAAExDMgoAAADTkIwCAADANCSjAAAAMA3JKAAAAExDMgoAAADT/B9ra4o0k5domgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_array, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/11 23:29:44 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:29:44 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:29:54 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:29:54 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:30:04 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:30:04 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:30:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:30:14 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:30:24 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:30:24 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:30:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/06/11 23:30:34 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hakancolaksair2:65496\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Is Fraud?\", predictionCol=\"prediction\")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"Is Fraud?\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "precision_label_0 = evaluator_precision.evaluate(predictions, {evaluator_precision.metricLabel: 0.0})\n",
    "precision_label_1 = evaluator_precision.evaluate(predictions, {evaluator_precision.metricLabel: 1.0})\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"Is Fraud?\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "recall_label_0 = evaluator_recall.evaluate(predictions, {evaluator_recall.metricLabel: 0.0})\n",
    "recall_label_1 = evaluator_recall.evaluate(predictions, {evaluator_recall.metricLabel: 1.0})\n",
    "\n",
    "\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(f\"Precision label 1: {precision_label_1}\")\n",
    "print(f\"Recall label 1: {recall_label_1}\")\n",
    "\n",
    "print(f\"Precision label 0: {precision_label_0}\")\n",
    "print(f\"Recall label 0: {recall_label_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**: 0.9160<br>\n",
    "**F1 Score**: 0.8903<br>\n",
    "**Precision label 1**: 0.6760<br>\n",
    "**Recall label 1**: 0.1428<br>\n",
    "**Precision label 0**: 0.9207<br>\n",
    "**Recall label 0**: 0.9931<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
